{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lP6JLo1tGNBg"
   },
   "source": [
    "# Perceptron multicapa: Clasificación\n",
    "En este notebook trabajaremos sobre predicción de Churn. Específicamente, decidir si una persona dejará un banco o no dadas sus características usando una red neuronal fully connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxkJoQBkUIHC"
   },
   "outputs": [],
   "source": [
    "# importar librerias necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1E0Q3aoKUCRX"
   },
   "source": [
    "## Parte 1. Manejo de datos\n",
    "### 1. Importar el conjunto de datos / EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MXUkhkMfU4wq"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "print(dataset.columns)\n",
    "print(dataset.info())\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Procesamiento de datos\n",
    "Primero removemos la variable dependiente `Exited` ya que esta representa la etiqueta. De esta manera separamos nuestros datos en atributos `X` y etiqueta `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2396,
     "status": "ok",
     "timestamp": 1590257449961,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "VYP9cQTWbzuI",
    "outputId": "797e7a64-9bac-436a-8c9c-94437e5e7587"
   },
   "outputs": [],
   "source": [
    "# TODO: Selecciona las características y la variable objetivo\n",
    "feature_cols = []\n",
    "X = dataset[feature_cols]\n",
    "y = dataset[]\n",
    "print(X.shape, y.shape, y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6bQ0UgSU-NJ"
   },
   "source": [
    "### Ingeniería de atributos\n",
    "\n",
    "Tenemos diferentes tipos de variables, algunas numéricas y otras de tipo categórico o `object` en pandas. Podemos usar diferentes tipos de codificadores para los categóricos:\n",
    "- [OneHotEncoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder)\n",
    "- [OrdinalEncoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder)\n",
    "\n",
    "Otras transformaciones comunes son la normalización:\n",
    "- [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PxVKWXxLbczC"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Diccionario de encoders feature: encoder para poder procesar los datos\n",
    "encoders = [\n",
    "    # handle_unknown='ignore' para evitar errores si hay categorias nuevas en test set\n",
    "    # unknown sera codificado como otra categoria\n",
    "    (\"gender_enc\", OneHotEncoder(handle_unknown='ignore'), [\"Gender\"]),\n",
    "    (\"geo_enc\", OrdinalEncoder(), [\"Geography\"]),\n",
    "]\n",
    "\n",
    "# Solo aplicamos los encoders a las columnas categóricas\n",
    "# las demas columnas se dejan igual (remainder='passthrough')\n",
    "preprocessor = ColumnTransformer(encoders, remainder='passthrough')\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Nota como X_processed es un numpy array, ya no tenemos los nombres de las columnas y no tenemos un pd dataframe\n",
    "print(X_processed[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionalmente, normalizamos los datos numéricos\n",
    "sc = StandardScaler()\n",
    "X_processed = sc.fit_transform(X_processed)\n",
    "\n",
    "print(\"Datos procesados y normalizados:\")\n",
    "print(X_processed[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vHol938cW8zd"
   },
   "source": [
    "### 3. Uniendo todo el manejo de datos...\n",
    "\n",
    "Para entrenar siempre tenemos que tener un split de train y validación. \n",
    "Vamos a primero generar el split y después aplicar el procesamiento de la celda anterior para evitar el data leakage.\n",
    "En la siguiente celda se resume el codigo anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-TDt0Y_XEfc"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# leer datos\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset[feature_cols]\n",
    "y = dataset['Exited']\n",
    "\n",
    "# Separar en train y test, 20% de los datos van a validacion\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Transformar a numpy arrays\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "# Preprocesamiento de datos\n",
    "def preprocess(X, train=False, encoders=None, scaler=None):\n",
    "    if train:\n",
    "        # TODO: Elige y aplica los encoders\n",
    "        encoders = [\n",
    "            (\"gender_enc\", ... , [\"Gender\"]),\n",
    "            (\"geo_enc\", ..., [\"Geography\"]),\n",
    "        ]\n",
    "        preprocessor = ColumnTransformer(encoders, remainder='passthrough')\n",
    "        X_encoded = preprocessor.fit_transform(X)\n",
    "        print(X_encoded[:1])\n",
    "        scaler = StandardScaler()\n",
    "        X_processed = scaler.fit_transform(X_encoded)\n",
    "\n",
    "        return X_processed, preprocessor, scaler\n",
    "    else:\n",
    "        X_encoded = encoders.transform(X)  # encoders = preprocessor ya entrenado\n",
    "        X_processed = scaler.transform(X_encoded)\n",
    "        return X_processed\n",
    "\n",
    "X_train_processed, preprocessor, scaler = preprocess(X_train, train=True)\n",
    "\n",
    "# Aplicar procesamiento de datos a validacion, ya no se usa fit_transform sino solo transform\n",
    "# le mandamos los encoders y el normalizador entrenado\n",
    "X_test_processed = preprocess(X_test, train=False, encoders=preprocessor, scaler=scaler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como visto en clase, los codificadores le asignan un valor numerico a un categórico. Para poder saber a que se asignó cada categórico corre la siguiente celda. También podras visualizar la información que guarda el normalizador.\n",
    "\n",
    "#TODO: Responde a que columna(s) corresponde la geografía?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_encoders(preprocessor, scaler):\n",
    "    print(\"Encoders usados:\")\n",
    "    for name, encoder, cols in preprocessor.transformers_:\n",
    "        if name != 'remainder':\n",
    "            print(f\"\\n- {name}: {encoder} en columnas {cols}\")\n",
    "            if hasattr(encoder, 'categories_'):  # OrdinalEncoder\n",
    "                print(f\"  Categorías: {encoder.categories_}\")\n",
    "            if hasattr(encoder, 'get_feature_names_out'):  # OneHotEncoder\n",
    "                print(f\"  Columnas codificadas: {encoder.get_feature_names_out(cols)}\")\n",
    "    \n",
    "    print(\"\\nScaler usado:\")\n",
    "    print(scaler)\n",
    "    \n",
    "    # Imprimir medias y desviaciones del StandardScaler\n",
    "    if hasattr(scaler, 'mean_') and hasattr(scaler, 'scale_'):\n",
    "        print(\"\\nMedias del StandardScaler:\")\n",
    "        print(scaler.mean_)\n",
    "        print(\"Desviaciones estándar del StandardScaler:\")\n",
    "        print(scaler.scale_)\n",
    "\n",
    "viz_encoders(preprocessor, scaler)\n",
    "\n",
    "def show_column_mapping(preprocessor):\n",
    "    col_index = 0\n",
    "    for name, transformer, cols in preprocessor.transformers_:\n",
    "        if name != 'remainder':\n",
    "            if hasattr(transformer, 'get_feature_names_out'):  # OneHot\n",
    "                out_cols = transformer.get_feature_names_out(cols)\n",
    "            else:  # Ordinal o 1 columna\n",
    "                out_cols = cols\n",
    "            for c in out_cols:\n",
    "                print(f\"Columna {col_index}: {c}\")\n",
    "                col_index += 1\n",
    "\n",
    "    # Columns passthrough\n",
    "    if preprocessor.remainder == 'passthrough':\n",
    "        passthrough_cols = preprocessor.transformers_[-1][-1]  # lista de columnas pasadas\n",
    "        # O si no se listan, agregarlas manualmente\n",
    "        print(\"Las columnas restantes (passthrough) se añaden al final\")\n",
    "\n",
    "show_column_mapping(preprocessor)\n",
    "\n",
    "print(X_train_processed[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-zfEzkRVXIwF"
   },
   "source": [
    "## Parte 2. Construyendo el modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KvdeScabXtlB"
   },
   "source": [
    "### Crear la red neuronal\n",
    "\n",
    "La red neuronal fully connected en pytorch se implementa de forma muy sencilla. Se escpefician los tamaños de las matrices de peso y la función de activación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3dtrScHxXQox"
   },
   "outputs": [],
   "source": [
    "# TODO: Completa la definición del modelo\n",
    "model = nn.Sequential(\n",
    "    # TODO: Responde, por que usamos X_train_processed.shape[1]?\n",
    "    nn.Linear(X_train_processed.shape[1], 6),\n",
    "    nn.ReLU(),\n",
    "    ...\n",
    "    # TODO: Selecciona una función de activación adecuada para salida binaria\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JT4u2S1_Y4WG"
   },
   "source": [
    "## Part 3. Entrenamiento\n",
    "\n",
    "Las redes neuronales se entrenan con descenso de gradiente. En la siguiente sección implementamos descenso de gradiente para aplicar a la red anterior en el conjunto de datos. Para esto se tiene que definir 3 elementos principales:\n",
    "1. El DataLoader. Este especifica como se van a ir cargando los datos y automatiza el proceso de separlo en minibatches\n",
    "2. El optimizador. este especifica la regla de actualización de los pesos en su implementación (Por eso aquí se especifica la tasa de aprendizaje)\n",
    "3. La función de costo. Esta especifica el costo del cual se calculará el gradiente. El optimizador utiliza esta función para guiar la actualización de los pesos. Tenemos que escoger una función apropiada al problema que queremos resolver, en este caso como es clasificación binaria se utiliza el BCELoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fG3RrwDXZEaS"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "print(X_train_processed.shape, y_train.shape)\n",
    "\n",
    "train_data = []\n",
    "for i in range(X_train_processed.shape[0]):\n",
    "   x_i = X_train_processed[i].astype(np.float32) # 11,\n",
    "\n",
    "   y_i = y_train[i].astype(np.float32)  # scalar\n",
    "   y_i = np.expand_dims(y_i, axis=-1)  # 1,\n",
    "\n",
    "   train_data.append([x_i, y_i])\n",
    "train_loader = DataLoader(train_data, batch_size=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0QR_G5u7ZLSM"
   },
   "source": [
    "### Descenso de gradeinte\n",
    "Una vez definido lo anterior simplemente realizamos descenso de gradiente sobre la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33685,
     "status": "ok",
     "timestamp": 1590257481284,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "nHZ-LKv_ZRb3",
    "outputId": "718cc4b0-b5aa-40f0-9b20-d3d31730a531"
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\tlosses = []\n",
    "\t# TODO: Completa el ciclo de entrenamiento\n",
    "\tfor x_batch, y_batch in train_loader:\n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t# Forward pass\n",
    "\t\tpredictions = ...\n",
    "\n",
    "\t\t# Calculo del loss\n",
    "\t\tloss = ...\n",
    "\n",
    "\t\t# Calculo del gradiente\n",
    "\t\t...\n",
    "\n",
    "\t\t# Actualizacion de pesos w = w - lr * grad\n",
    "\t\t...\n",
    "\n",
    "\t\t# Agregar loss de este batch a la lista para promediar\n",
    "\t\tlosses.append(loss.item())\n",
    "\n",
    "\t\t# Agregar un step de valoración si se desea\n",
    "\t\t# val_step(model, X_val, y_val)\n",
    "\tprint(f\"epoch {epoch}: training loss {np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJj5k2MxZga3"
   },
   "source": [
    "## Parte 4. Inferencia\n",
    "\n",
    "Ahora que vimos como entrenar el modelo intenta predecir para un solo cliente de \"prueba\". Utiliza la red neuronal para decidir si el siguiente cliente dejará el banco.\n",
    "- Geography: France\n",
    "- Credit Score: 600\n",
    "- Gender: Male\n",
    "- Age: 40 years old\n",
    "- Tenure: 3 years\n",
    "- Balance: \\$ 60000\n",
    "- Number of Products: 2\n",
    "- Does this customer have a credit card? Yes\n",
    "- Is this customer an Active Member: Yes\n",
    "- Estimated Salary: \\$ 50000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZhU1LTgPg-kH"
   },
   "source": [
    "**Solución**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_cols)\n",
    "show_column_mapping(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33990,
     "status": "ok",
     "timestamp": 1590257481594,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "2d8IoCCkeWGL",
    "outputId": "957f3970-e197-4c3b-a150-7f69dc567f5d"
   },
   "outputs": [],
   "source": [
    "# Creamos un dato de prueba\n",
    "# Las primeras son del genero (one hot), despues el pais, las demas en el orden original.\n",
    "# La cantidad de columnas de genero y pais dependera de como fueron codificadas.\n",
    "# TODO: inicializa el tensor con datos de prueba\n",
    "inp_tensor = torch.from_numpy(sc.transform([[]])).float()\n",
    "\n",
    "# Hacemos la predicción\n",
    "pred = model(inp_tensor)\n",
    "\n",
    "# impromimir si es mayor a 0.5 (clase positiva)\n",
    "print(model(inp_tensor) > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wGjx94g2n7OV"
   },
   "source": [
    "Por lo tanto, ¡nuestro modelo de redes neuronales predice que este cliente se queda en el banco!\n",
    "\n",
    "**Nota importante 1**: Observa que los valores de las características se ingresaron todos dentro de un doble par de corchetes. Esto se debe a que el método predict siempre espera un array 2D como formato de entrada. Y al poner nuestros valores dentro de un doble par de corchetes, hacemos que la entrada sea exactamente un array 2D.\n",
    "\n",
    "**Nota importante 2**: Observa también que el país \"France\" no se ingresó como una cadena en la última columna. Esto se debe a que, por supuesto, el método predict espera los valores segun se haya codificado el país, identifica como se codificó Francia. Y ten cuidado de incluir estos valores en las primeras tres columnas, porque las variables dummy siempre se crean en las primeras columnas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u7yx47jPZt11"
   },
   "source": [
    "### Prediciendo resultados de validación\n",
    "Lo comun durante entrenamiento es en cada epoch evaluar TODO el conjunto de validación. En la celda de abajo se muestra una evaluación.\n",
    "\n",
    "Normalmente esto estaría implementado dentro de un método y estaría dentro del for loop de descenso de gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33987,
     "status": "ok",
     "timestamp": 1590257481595,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "nIyEeQdRZwgs",
    "outputId": "82330ba8-9bdc-4fd1-d3cf-b6d78ee7c2a3"
   },
   "outputs": [],
   "source": [
    "X_test_torch = torch.from_numpy(X_test_processed).float()\n",
    "y_test_torch = torch.from_numpy(y_test).float()\n",
    "\n",
    "y_pred = model(X_test_torch)\n",
    "y_pred = (y_pred > 0.5).numpy()\n",
    "\n",
    "print(y_pred.shape, y_test.shape)\n",
    "\n",
    "# Resultados, accoracy\n",
    "y_test_reshaped = y_test.reshape(-1, 1)\n",
    "print(y_test_reshaped.shape)\n",
    "\n",
    "# TODO: Calcula la accuracy comparando y_pred y y_test_reshaped\n",
    "accuracy = ...\n",
    "\n",
    "print(f\"Accuracy de validación: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otras métricas que podemos calcular son la matriz de confusión o usar las funciones de sklearn para las métricas en lugar de implementarlas directamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33981,
     "status": "ok",
     "timestamp": 1590257481595,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "ci6K_r6LaF6P",
    "outputId": "4d854e9e-22d5-432f-f6e5-a102fe3ae0bd"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMeRFWFoGrdaL5S3dx5MWmb",
   "collapsed_sections": [],
   "name": "artificial_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
