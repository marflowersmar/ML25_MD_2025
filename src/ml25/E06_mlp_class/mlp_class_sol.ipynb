{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lP6JLo1tGNBg"
   },
   "source": [
    "# Perceptron multicapa: Clasificación\n",
    "En este notebook trabajaremos sobre predicción de Churn. Específicamente, decidir si una persona dejará un banco o no dadas sus características usando una red neuronal fully connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxkJoQBkUIHC"
   },
   "outputs": [],
   "source": [
    "# importar librerias necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1E0Q3aoKUCRX"
   },
   "source": [
    "## Parte 1. Manejo de datos\n",
    "### 1. Importar el conjunto de datos / EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MXUkhkMfU4wq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['RowNumber', 'CustomerId', 'Surname', 'CreditScore', 'Geography',\n",
      "       'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n",
      "       'IsActiveMember', 'EstimatedSalary', 'Exited'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           10000 non-null  int64  \n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(2), int64(9), object(3)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
      "0          1    15634602  Hargrave          619    France  Female   42   \n",
      "1          2    15647311      Hill          608     Spain  Female   41   \n",
      "2          3    15619304      Onio          502    France  Female   42   \n",
      "3          4    15701354      Boni          699    France  Female   39   \n",
      "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
      "\n",
      "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
      "0       2       0.00              1          1               1   \n",
      "1       1   83807.86              1          0               1   \n",
      "2       8  159660.80              3          1               0   \n",
      "3       1       0.00              2          0               0   \n",
      "4       2  125510.82              1          1               1   \n",
      "\n",
      "   EstimatedSalary  Exited  \n",
      "0        101348.88       1  \n",
      "1        112542.58       0  \n",
      "2        113931.57       1  \n",
      "3         93826.63       0  \n",
      "4         79084.10       0  \n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "print(dataset.columns)\n",
    "print(dataset.info())\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Procesamiento de datos\n",
    "Primero removemos la variable dependiente `Exited` ya que esta representa la etiqueta. De esta manera separamos nuestros datos en atributos `X` y etiqueta `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2396,
     "status": "ok",
     "timestamp": 1590257449961,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "VYP9cQTWbzuI",
    "outputId": "797e7a64-9bac-436a-8c9c-94437e5e7587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10) (10000,) 0    1\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "4    0\n",
      "5    1\n",
      "6    0\n",
      "7    1\n",
      "8    0\n",
      "9    0\n",
      "Name: Exited, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "feature_cols = ['CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "X = dataset[feature_cols]\n",
    "y = dataset['Exited']\n",
    "print(X.shape, y.shape, y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6bQ0UgSU-NJ"
   },
   "source": [
    "### Ingeniería de atributos\n",
    "\n",
    "Tenemos diferentes tipos de variables, algunas numéricas y otras de tipo categórico o `object` en pandas. Podemos usar diferentes tipos de codificadores para los categóricos:\n",
    "- [OneHotEncoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder)\n",
    "- [OrdinalEncoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder)\n",
    "\n",
    "Otras transformaciones comunes son la normalización:\n",
    "- [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PxVKWXxLbczC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000000e+00 0.0000000e+00 0.0000000e+00 6.1900000e+02 4.2000000e+01\n",
      "  2.0000000e+00 0.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00\n",
      "  1.0134888e+05]\n",
      " [1.0000000e+00 0.0000000e+00 2.0000000e+00 6.0800000e+02 4.1000000e+01\n",
      "  1.0000000e+00 8.3807860e+04 1.0000000e+00 0.0000000e+00 1.0000000e+00\n",
      "  1.1254258e+05]\n",
      " [1.0000000e+00 0.0000000e+00 0.0000000e+00 5.0200000e+02 4.2000000e+01\n",
      "  8.0000000e+00 1.5966080e+05 3.0000000e+00 1.0000000e+00 0.0000000e+00\n",
      "  1.1393157e+05]\n",
      " [1.0000000e+00 0.0000000e+00 0.0000000e+00 6.9900000e+02 3.9000000e+01\n",
      "  1.0000000e+00 0.0000000e+00 2.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  9.3826630e+04]\n",
      " [1.0000000e+00 0.0000000e+00 2.0000000e+00 8.5000000e+02 4.3000000e+01\n",
      "  2.0000000e+00 1.2551082e+05 1.0000000e+00 1.0000000e+00 1.0000000e+00\n",
      "  7.9084100e+04]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Diccionario de encoders feature: encoder para poder procesar los datos\n",
    "encoders = [\n",
    "    # handle_unknown='ignore' para evitar errores si hay categorias nuevas en test set\n",
    "    # unknown sera codificado como otra categoria\n",
    "    (\"gender_enc\", OneHotEncoder(handle_unknown='ignore'), [\"Gender\"]),\n",
    "    (\"geo_enc\", OrdinalEncoder(), [\"Geography\"]),\n",
    "]\n",
    "\n",
    "# Solo aplicamos los encoders a las columnas categóricas\n",
    "# las demas columnas se dejan igual (remainder='passthrough')\n",
    "preprocessor = ColumnTransformer(encoders, remainder='passthrough')\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Nota como X_processed es un numpy array, ya no tenemos los nombres de las columnas y no tenemos un pd dataframe\n",
    "print(X_processed[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos procesados y normalizados:\n",
      "[[ 1.09598752 -1.09598752 -0.90188624 -0.32622142  0.29351742 -1.04175968\n",
      "  -1.22584767 -0.91158349  0.64609167  0.97024255  0.02188649]\n",
      " [ 1.09598752 -1.09598752  1.51506738 -0.44003595  0.19816383 -1.38753759\n",
      "   0.11735002 -0.91158349 -1.54776799  0.97024255  0.21653375]\n",
      " [ 1.09598752 -1.09598752 -0.90188624 -1.53679418  0.29351742  1.03290776\n",
      "   1.33305335  2.52705662  0.64609167 -1.03067011  0.2406869 ]\n",
      " [ 1.09598752 -1.09598752 -0.90188624  0.50152063  0.00745665 -1.38753759\n",
      "  -1.22584767  0.80773656 -1.54776799 -1.03067011 -0.10891792]\n",
      " [ 1.09598752 -1.09598752  1.51506738  2.06388377  0.38887101 -1.04175968\n",
      "   0.7857279  -0.91158349  0.64609167  0.97024255 -0.36527578]]\n"
     ]
    }
   ],
   "source": [
    "# Adicionalmente, normalizamos los datos numéricos\n",
    "sc = StandardScaler()\n",
    "X_processed = sc.fit_transform(X_processed)\n",
    "\n",
    "print(\"Datos procesados y normalizados:\")\n",
    "print(X_processed[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vHol938cW8zd"
   },
   "source": [
    "### 3. Uniendo todo el manejo de datos...\n",
    "\n",
    "Para entrenar siempre tenemos que tener un split de train y validación. \n",
    "Vamos a primero generar el split y después aplicar el procesamiento de la celda anterior para evitar el data leakage.\n",
    "En la siguiente celda se resume el codigo anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-TDt0Y_XEfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 13)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# leer datos\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset[feature_cols]\n",
    "y = dataset['Exited']\n",
    "\n",
    "# Separar en train y test, 20% de los datos van a validacion\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Transformar a numpy arrays\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "# Preprocesamiento de datos\n",
    "def preprocess(X, train=False, encoders=None, scaler=None):\n",
    "    if train:\n",
    "        encoders = [\n",
    "            (\"gender_enc\", OneHotEncoder(handle_unknown='ignore'), [\"Gender\"]),\n",
    "            (\"geo_enc\", OneHotEncoder(handle_unknown='ignore'), [\"Geography\"]),\n",
    "        ]\n",
    "        preprocessor = ColumnTransformer(encoders, remainder='passthrough')\n",
    "        X_encoded = preprocessor.fit_transform(X)\n",
    "        print(X_encoded.shape)\n",
    "        scaler = StandardScaler()\n",
    "        X_processed = scaler.fit_transform(X_encoded)\n",
    "\n",
    "        return X_processed, preprocessor, scaler\n",
    "    else:\n",
    "        X_encoded = encoders.transform(X)  # encoders = preprocessor ya entrenado\n",
    "        X_processed = scaler.transform(X_encoded)\n",
    "        return X_processed\n",
    "\n",
    "X_train_processed, preprocessor, scaler = preprocess(X_train, train=True)\n",
    "\n",
    "# Aplicar procesamiento de datos a validacion, ya no se usa fit_transform sino solo transform\n",
    "# le mandamos los encoders y el normalizador entrenado\n",
    "X_test_processed = preprocess(X_test, train=False, encoders=preprocessor, scaler=scaler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como visto en clase, los codificadores le asignan un valor numerico a un categórico. Para poder saber a que se asignó cada categórico corre la siguiente celda. También podras visualizar la información que guarda el normalizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoders usados:\n",
      "\n",
      "- gender_enc: OneHotEncoder(handle_unknown='ignore') en columnas ['Gender']\n",
      "  Categorías: [array(['Female', 'Male'], dtype=object)]\n",
      "  Columnas codificadas: ['Gender_Female' 'Gender_Male']\n",
      "\n",
      "- geo_enc: OneHotEncoder(handle_unknown='ignore') en columnas ['Geography']\n",
      "  Categorías: [array(['France', 'Germany', 'Spain'], dtype=object)]\n",
      "  Columnas codificadas: ['Geography_France' 'Geography_Germany' 'Geography_Spain']\n",
      "\n",
      "Scaler usado:\n",
      "StandardScaler()\n",
      "\n",
      "Medias del StandardScaler:\n",
      "[4.56250000e-01 5.43750000e-01 5.07250000e-01 2.45125000e-01\n",
      " 2.47625000e-01 6.50550000e+02 3.88546250e+01 4.98075000e+00\n",
      " 7.60765042e+04 1.53237500e+00 7.07750000e-01 5.15875000e-01\n",
      " 1.00172283e+05]\n",
      "Desviaciones estándar del StandardScaler:\n",
      "[4.98082260e-01 4.98082260e-01 4.99947435e-01 4.30161289e-01\n",
      " 4.31632783e-01 9.70033556e+01 1.04488631e+01 2.88996184e+00\n",
      " 6.25774532e+04 5.77669334e-01 4.54796589e-01 4.99747921e-01\n",
      " 5.75348298e+04]\n",
      "Columna 0: Gender_Female\n",
      "Columna 1: Gender_Male\n",
      "Columna 2: Geography_France\n",
      "Columna 3: Geography_Germany\n",
      "Columna 4: Geography_Spain\n",
      "Las columnas restantes (passthrough) se añaden al final\n",
      "[[ 1.09168714 -1.09168714 -1.01460667 -0.5698444   1.74309049  0.16958176\n",
      "  -0.46460796  0.00666099 -1.21571749  0.8095029   0.64259497 -1.03227043\n",
      "   1.10643166]]\n"
     ]
    }
   ],
   "source": [
    "def viz_encoders(preprocessor, scaler):\n",
    "    print(\"Encoders usados:\")\n",
    "    for name, encoder, cols in preprocessor.transformers_:\n",
    "        if name != 'remainder':\n",
    "            print(f\"\\n- {name}: {encoder} en columnas {cols}\")\n",
    "            if hasattr(encoder, 'categories_'):  # OrdinalEncoder\n",
    "                print(f\"  Categorías: {encoder.categories_}\")\n",
    "            if hasattr(encoder, 'get_feature_names_out'):  # OneHotEncoder\n",
    "                print(f\"  Columnas codificadas: {encoder.get_feature_names_out(cols)}\")\n",
    "    \n",
    "    print(\"\\nScaler usado:\")\n",
    "    print(scaler)\n",
    "    \n",
    "    # Imprimir medias y desviaciones del StandardScaler\n",
    "    if hasattr(scaler, 'mean_') and hasattr(scaler, 'scale_'):\n",
    "        print(\"\\nMedias del StandardScaler:\")\n",
    "        print(scaler.mean_)\n",
    "        print(\"Desviaciones estándar del StandardScaler:\")\n",
    "        print(scaler.scale_)\n",
    "\n",
    "viz_encoders(preprocessor, scaler)\n",
    "\n",
    "def show_column_mapping(preprocessor):\n",
    "    col_index = 0\n",
    "    for name, transformer, cols in preprocessor.transformers_:\n",
    "        if name != 'remainder':\n",
    "            if hasattr(transformer, 'get_feature_names_out'):  # OneHot\n",
    "                out_cols = transformer.get_feature_names_out(cols)\n",
    "            else:  # Ordinal o 1 columna\n",
    "                out_cols = cols\n",
    "            for c in out_cols:\n",
    "                print(f\"Columna {col_index}: {c}\")\n",
    "                col_index += 1\n",
    "\n",
    "    # Columns passthrough\n",
    "    if preprocessor.remainder == 'passthrough':\n",
    "        passthrough_cols = preprocessor.transformers_[-1][-1]  # lista de columnas pasadas\n",
    "        # O si no se listan, agregarlas manualmente\n",
    "        print(\"Las columnas restantes (passthrough) se añaden al final\")\n",
    "\n",
    "show_column_mapping(preprocessor)\n",
    "\n",
    "print(X_train_processed[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-zfEzkRVXIwF"
   },
   "source": [
    "## Parte 2. Construyendo el modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KvdeScabXtlB"
   },
   "source": [
    "### Crear la red neuronal\n",
    "\n",
    "La red neuronal fully connected en pytorch se implementa de forma muy sencilla. Se escpefician los tamaños de las matrices de peso y la función de activación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3dtrScHxXQox"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(X_train_processed.shape[1], 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JT4u2S1_Y4WG"
   },
   "source": [
    "## Part 3. Entrenamiento\n",
    "\n",
    "Las redes neuronales se entrenan con descenso de gradiente. En la siguiente sección implementamos descenso de gradiente para aplicar a la red anterior en el conjunto de datos. Para esto se tiene que definir 3 elementos principales:\n",
    "1. El DataLoader. Este especifica como se van a ir cargando los datos y automatiza el proceso de separlo en minibatches\n",
    "2. El optimizador. este especifica la regla de actualización de los pesos en su implementación (Por eso aquí se especifica la tasa de aprendizaje)\n",
    "3. La función de costo. Esta especifica el costo del cual se calculará el gradiente. El optimizador utiliza esta función para guiar la actualización de los pesos. Tenemos que escoger una función apropiada al problema que queremos resolver, en este caso como es clasificación binaria se utiliza el BCELoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fG3RrwDXZEaS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 13) (8000,)\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "print(X_train_processed.shape, y_train.shape)\n",
    "\n",
    "train_data = []\n",
    "for i in range(X_train_processed.shape[0]):\n",
    "   x_i = X_train_processed[i].astype(np.float32) # 11,\n",
    "\n",
    "   y_i = y_train[i].astype(np.float32)  # scalar\n",
    "   y_i = np.expand_dims(y_i, axis=-1)  # 1,\n",
    "\n",
    "   train_data.append([x_i, y_i])\n",
    "train_loader = DataLoader(train_data, batch_size=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0QR_G5u7ZLSM"
   },
   "source": [
    "### Descenso de gradeinte\n",
    "Una vez definido lo anterior simplemente realizamos descenso de gradiente sobre la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33685,
     "status": "ok",
     "timestamp": 1590257481284,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "nHZ-LKv_ZRb3",
    "outputId": "718cc4b0-b5aa-40f0-9b20-d3d31730a531"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: training loss 0.395110987027421\n",
      "epoch 1: training loss 0.3567352691536553\n",
      "epoch 2: training loss 0.3461608087948718\n",
      "epoch 3: training loss 0.3405612688609387\n",
      "epoch 4: training loss 0.3374840562099025\n",
      "epoch 5: training loss 0.33166928253970124\n",
      "epoch 6: training loss 0.33224859811880564\n",
      "epoch 7: training loss 0.3269090829897845\n",
      "epoch 8: training loss 0.32175128092363553\n",
      "epoch 9: training loss 0.3198302005200092\n",
      "epoch 10: training loss 0.3146617976135443\n",
      "epoch 11: training loss 0.30973480072374393\n",
      "epoch 12: training loss 0.3068600448495395\n",
      "epoch 13: training loss 0.3015042412969058\n",
      "epoch 14: training loss 0.29445923983711386\n",
      "epoch 15: training loss 0.28952852824551306\n",
      "epoch 16: training loss 0.28446335412524415\n",
      "epoch 17: training loss 0.27252235796550256\n",
      "epoch 18: training loss 0.2696747699569526\n",
      "epoch 19: training loss 0.2628941094048145\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\tlosses = []\n",
    "\tfor x_batch, y_batch in train_loader:\n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t# Forward pass\n",
    "\t\tpredictions = model(x_batch)\n",
    "\n",
    "\t\t# Calculo del loss\n",
    "\t\tloss = loss_fn(predictions, y_batch)\n",
    "\n",
    "\t\t# Calculo del gradiente\n",
    "\t\tloss.backward()\n",
    "\n",
    "\t\t# Actualizacion de pesos w = w - lr * grad\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\t# Agregar loss de este batch a la lista para promediar\n",
    "\t\tlosses.append(loss.item())\n",
    "\n",
    "\t\t# Agregar un step de valoración si se desea\n",
    "\t\t# val_step(model, X_val, y_val)\n",
    "\tprint(f\"epoch {epoch}: training loss {np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJj5k2MxZga3"
   },
   "source": [
    "## Parte 4. Inferencia\n",
    "\n",
    "Ahora que vimos como entrenar el modelo intenta predecir para un solo cliente de \"prueba\". Utiliza la red neuronal para decidir si el siguiente cliente dejará el banco.\n",
    "- Geography: France\n",
    "- Credit Score: 600\n",
    "- Gender: Male\n",
    "- Age: 40 years old\n",
    "- Tenure: 3 years\n",
    "- Balance: \\$ 60000\n",
    "- Number of Products: 2\n",
    "- Does this customer have a credit card? Yes\n",
    "- Is this customer an Active Member: Yes\n",
    "- Estimated Salary: \\$ 50000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZhU1LTgPg-kH"
   },
   "source": [
    "**Solución**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
      "Columna 0: Gender_Female\n",
      "Columna 1: Gender_Male\n",
      "Columna 2: Geography_France\n",
      "Columna 3: Geography_Germany\n",
      "Columna 4: Geography_Spain\n",
      "Las columnas restantes (passthrough) se añaden al final\n"
     ]
    }
   ],
   "source": [
    "print(feature_cols)\n",
    "show_column_mapping(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33990,
     "status": "ok",
     "timestamp": 1590257481594,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "2d8IoCCkeWGL",
    "outputId": "957f3970-e197-4c3b-a150-7f69dc567f5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False]])\n"
     ]
    }
   ],
   "source": [
    "# Creamos un dato de prueba\n",
    "# Las primeras dos son el genero (one hot) y la tercera el pais (ordinal), las demas en el orden original\n",
    "arr = np.array([[0, 1, 1, 0, 0, 600, 40, 3, 60000, 2, 1, 1, 50000]])\n",
    "\n",
    "# Scaler es el ultimo que se entreno\n",
    "processed = scaler.transform(arr)\n",
    "inp_tensor = torch.from_numpy(processed).float()\n",
    "\n",
    "# Hacemos la predicción\n",
    "pred = model(inp_tensor)\n",
    "\n",
    "# impromimir si es mayor a 0.5 (clase positiva)\n",
    "print(model(inp_tensor) > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wGjx94g2n7OV"
   },
   "source": [
    "Por lo tanto, ¡nuestro modelo de redes neuronales predice que este cliente se queda en el banco! (false significa no hace churn)\n",
    "\n",
    "**Nota importante 1**: Observa que los valores de las características se ingresaron todos dentro de un doble par de corchetes. Esto se debe a que el método predict siempre espera un array 2D como formato de entrada. Y al poner nuestros valores dentro de un doble par de corchetes, hacemos que la entrada sea exactamente un array 2D.\n",
    "\n",
    "**Nota importante 2**: Observa también que el país \"France\" no se ingresó como una cadena en la última columna, sino como \"1, 0, 0\" en las tres primeras columnas. Esto se debe a que, por supuesto, el método predict espera los valores one-hot encoded del estado, y como vemos en la primera fila de la matriz de características X, \"France\" se codificó como \"1, 0, 0\". Y ten cuidado de incluir estos valores en las primeras tres columnas, porque las variables dummy siempre se crean en las primeras columnas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u7yx47jPZt11"
   },
   "source": [
    "### Prediciendo resultados de validación\n",
    "Lo comun durante entrenamiento es en cada epoch evaluar TODO el conjunto de validación. En la celda de abajo se muestra una evaluación.\n",
    "\n",
    "Normalmente esto estaría implementado dentro de un método y estaría dentro del for loop de descenso de gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33987,
     "status": "ok",
     "timestamp": 1590257481595,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "nIyEeQdRZwgs",
    "outputId": "82330ba8-9bdc-4fd1-d3cf-b6d78ee7c2a3"
   },
   "outputs": [],
   "source": [
    "X_test_torch = torch.from_numpy(X_test_processed).float()\n",
    "y_test_torch = torch.from_numpy(y_test).float()\n",
    "\n",
    "y_pred = model(X_test_torch)\n",
    "y_pred = (y_pred > 0.5).numpy()\n",
    "\n",
    "print(y_pred.shape, y_test.shape)\n",
    "\n",
    "# Resultados, accoracy\n",
    "y_test_reshaped = y_test.reshape(-1, 1)\n",
    "print(y_test_reshaped.shape)\n",
    "accuracy = np.mean(y_pred == y_test_reshaped)\n",
    "print(f\"Accuracy de validación: {accuracy * 100:.2f}%\")\n",
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otras métricas que podemos calcular son la matriz de confusión o usar las funciones de sklearn para las métricas en lugar de implementarlas directamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33981,
     "status": "ok",
     "timestamp": 1590257481595,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "ci6K_r6LaF6P",
    "outputId": "4d854e9e-22d5-432f-f6e5-a102fe3ae0bd"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMeRFWFoGrdaL5S3dx5MWmb",
   "collapsed_sections": [],
   "name": "artificial_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
